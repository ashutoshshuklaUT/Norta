{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803a776a-86f1-4595-bf25-baac52ab0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from norta import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347b54b7-5382-47f6-b5c8-fc142bde7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_heuristic():\n",
    "    df = pd.read_csv(\"/Users/ashutoshshukla/Desktop/Data/fixed_reduced_grid/192_Scenario/Final_Input1.csv\")\n",
    "    \n",
    "    # directions = [\"w\", \"wnw\", \"nw\", \"nnw\", \"n\", \"nne\"]\n",
    "    # categories = [\"2\", \"3\", \"4\", \"5\"]\n",
    "    # forward_speeds = [\"05\", \"10\", \"15\", \"25\"]\n",
    "\n",
    "    directions = [\"w\", \"wnw\", \"nw\", \"nnw\"]\n",
    "    categories = [\"5\"]\n",
    "    forward_speeds = [\"05\", \"10\", \"15\", \"25\"]\n",
    "    \n",
    "    lister = []\n",
    "    for i in directions:\n",
    "        for j in range(len(categories)):\n",
    "            for k in range(len(forward_speeds)):\n",
    "                lister.append(\"max_flood_level_\" + i +\"_\" + categories[j] + \"_\" + forward_speeds[k])\n",
    "    df = df[list(df.columns[0:9]) + lister]\n",
    "    df_sub = df[[\"SubNum\", \"load\"]].groupby(\"SubNum\").sum()\n",
    "    df_flood = df[[\"SubNum\"] + lister]\n",
    "    df_flood = df_flood.drop_duplicates().set_index(\"SubNum\") # drop duplicates\n",
    "    df_flood = df_flood.loc[(df_flood.sum(axis=1) != 0), :] # drop substations that are not flooded\n",
    "    \"\"\"df_sub has load demand for all the substations\"\"\"\n",
    "    \"\"\"df_flood has only flooded substations\"\"\"\n",
    "    return df, df_sub, df_flood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60b4bc-351e-465b-b5d8-775ff760653f",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3a13d-a113-44f0-872f-f9fb4fc7ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 5\n",
    "end = 9\n",
    "\n",
    "df, df_sub, df_flood = get_df_for_heuristic()\n",
    "df_flood = df_flood.T\n",
    "data = df_flood.iloc[:,start:end].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac065f5-4b24-4a05-92d2-8b7f8402c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    norta_data = fit_NORTA(data, data.shape[0], data.shape[1], np.cov(data, rowvar=False), \n",
    "                           lambda_param=0.005, mc_samples=2E6, output_flag=1, n_proc=4)\n",
    "\n",
    "    parent_folder = \"output\"    \n",
    "    new_directory = \"flood_\" + str(data.shape[1]) + \"d\"\n",
    "\n",
    "    new_directory_path = os.path.join(parent_folder, new_directory)\n",
    "    os.makedirs(new_directory_path, exist_ok=True)\n",
    "\n",
    "    with open(new_directory_path + \"/norta_object\", 'wb') as file:\n",
    "        pickle.dump(norta_data, file)\n",
    "    pd.DataFrame(np.cov(data, rowvar=False)).to_csv(new_directory_path + \"/target_covariance.csv\")\n",
    "    pd.DataFrame(data).corr().to_csv(new_directory_path + \"/target_correlation.csv\", index=None)\n",
    "    df_flood.iloc[:,start:end].to_csv(new_directory_path + \"/source_df.csv\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Print the elapsed time\n",
    "    print(f\"Time taken to run the script: {elapsed_time} seconds\")\n",
    "    print(\"Dimension of the problem is: \", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de94656-c3a4-48bd-812d-b93f006f86c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_directory_path + \"/norta_object\", 'rb') as f:\n",
    "    # Load the object from the pickle file\n",
    "    norta_data = pickle.load(f)\n",
    "target_corr = pd.read_csv(new_directory_path + \"/target_correlation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4864e-7758-4bde-a44c-ef67b872cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d12b9-ef35-4a90-aace-699d62acc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = norta_data.gen(100000)\n",
    "pd.DataFrame(ng).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b552a-4b2c-421a-a01a-49f3c19b9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ng).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576f4cc-f3c7-4fe8-8789-967f221af10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac421c40-fefa-4f8f-9b91-f9906a44611c",
   "metadata": {},
   "source": [
    "### Scenario generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75afad6b-56eb-4ac8-a360-1266c0d4b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_sub, df_flood = get_df_for_heuristic()\n",
    "df_flood = df_flood.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85168e3f-e435-44a9-97c4-75f2366c5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory_path = \"/Users/ashutoshshukla/Desktop/Norta/output/precise_flood_72d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e309f44-20e1-469b-bc18-aceb31a0f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_directory_path + \"/norta_object\", 'rb') as f:\n",
    "    norta_data = pickle.load(f)\n",
    "\n",
    "target_corr = pd.read_csv(new_directory_path + \"/target_correlation.csv\")\n",
    "target_df = pd.read_csv(new_directory_path + \"/source_df.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965c7373-f77e-432d-bd04-a009893c6609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n",
      "Folder already exists.\n"
     ]
    }
   ],
   "source": [
    "scenario_ids = np.arange(50)\n",
    "maps_per_scenario = 16\n",
    "\n",
    "ng = 0\n",
    "for a_scenario_id in scenario_ids:\n",
    "    norta_data.reset_seed(a_scenario_id)\n",
    "    ng = norta_data.gen(maps_per_scenario)\n",
    "    ng  = pd.DataFrame(ng)\n",
    "    ng.columns = target_df.columns\n",
    "    ng.index = target_df.index\n",
    "\n",
    "    scenarios_folder = \"scenarios\"\n",
    "    folder_path = os.path.join(scenarios_folder, str(a_scenario_id))\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    else:\n",
    "        print(\"Folder already exists.\")    \n",
    "    ng.to_csv(\"/Users/ashutoshshukla/Desktop/Norta/scenarios/\" + str(a_scenario_id) + \"/\" + \"Final_Input1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fe6b7-7d0b-4d76-844a-102f46e24e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
